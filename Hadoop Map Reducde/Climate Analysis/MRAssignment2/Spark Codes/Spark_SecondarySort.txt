package org.test.spark
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object secondarySort {
  def main(args: Array[String]) = {
   //Start the Spark context
    
    val conf = new SparkConf()
      .setAppName("SecondarySort")
      .setMaster("local")
      val sc = new SparkContext(conf)
    val pairRDD = sc.textFile("/Users/chandrikasharma/Downloads/SecSordFolder”)

    val maxTemps = pairRDD.map(line => line.split(","))
                          .filter(fields => fields(2) =="TMAX" ||fields(2) =="TMIN")
                          .map(fields => ((fields(0),fields(2)),(fields(1), 	Integer.parseInt(fields(3)))))
//—————————————using GroupByKey

    val secSort =  maxTemps.groupByKey()
                            .sortBy(_._1, ascending=true)

//—————————————using RedeuceByKey

    val sumByKey=maxTemps.reduceByKey((sum,temp) => temp + sum) 

//—————————————using foldByKey                          

    val countByKey = maxTemps.foldByKey(0)((ct, temp) => ct + 1)

//—————————————using sumByKey

    val sumCountByKey = sumByKey.join(countByKey)

                        
    val avgByKey = sumCountByKey.map(t => (t._1, t._2._1 / t._2._2.toDouble))
    

    val maxKeyVal =  avgByKey.filter(t=>(t._1._2)=="TMAX").map(t=>(t._1._1,t._2))
    
    val minKeyVal =  avgByKey.filter(t=>(t._1._2)=="TMIN").map(t=>(t._1._1,t._2))
    val result = maxKeyVal.fullOuterJoin(minKeyVal).map {
              case (id, (left, right)) =>(id, (left.getOrElse(0.00)).toString + ( ","+ right.getOrElse(0.00)).toString)}
    
    
    
    result.saveAsTextFile("SecondarySort")
    sc.stop

  }
}